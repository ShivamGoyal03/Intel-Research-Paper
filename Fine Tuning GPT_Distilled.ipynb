{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "43cfe6d85fb24707be4bf7e8f62892cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3932927eff314cba8f8ce51d44462b7a",
              "IPY_MODEL_569e409d5d42456096768733ef2bbdc0",
              "IPY_MODEL_17e17af478c64ab1a35b9edb1af12670"
            ],
            "layout": "IPY_MODEL_ed689824cfcb4ee5ae7055336416fb7a"
          }
        },
        "3932927eff314cba8f8ce51d44462b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4379996f2e5748e093a76304d48baf39",
            "placeholder": "​",
            "style": "IPY_MODEL_a83c2902ea4247cdb57b6317100f16df",
            "value": "Generating train split: "
          }
        },
        "569e409d5d42456096768733ef2bbdc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57493f6191554cd28fd076f51b4ccb7c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22f89c9efffd484da82ac73cae6a3985",
            "value": 1
          }
        },
        "17e17af478c64ab1a35b9edb1af12670": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20734f8822fa4aa7b881873139912e73",
            "placeholder": "​",
            "style": "IPY_MODEL_5b13cad825994543af322ace3fc408a2",
            "value": " 743/0 [00:01&lt;00:00, 546.58 examples/s]"
          }
        },
        "ed689824cfcb4ee5ae7055336416fb7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4379996f2e5748e093a76304d48baf39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a83c2902ea4247cdb57b6317100f16df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57493f6191554cd28fd076f51b4ccb7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "22f89c9efffd484da82ac73cae6a3985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20734f8822fa4aa7b881873139912e73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b13cad825994543af322ace3fc408a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rVcCay0yC1xU",
        "outputId": "7a73d362-fc19-4e85-a8e4-85ac276fd2bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.0)\n",
            "Collecting openvino\n",
            "  Downloading openvino-2024.3.0-16041-cp310-cp310-manylinux2014_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting peft\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Collecting openvino-dev[onnx]\n",
            "  Downloading openvino_dev-2024.3.0-16041-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from openvino-dev[onnx]) (0.7.1)\n",
            "Collecting networkx (from torch)\n",
            "  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting openvino-telemetry>=2023.2.1 (from openvino-dev[onnx])\n",
            "  Downloading openvino_telemetry-2024.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting numpy>=1.17 (from transformers)\n",
            "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting fastjsonschema<2.18,>=2.15.1 (from openvino-dev[onnx])\n",
            "  Downloading fastjsonschema-2.17.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.8.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.8.0)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Downloading openvino-2024.3.0-16041-cp310-cp310-manylinux2014_x86_64.whl (40.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastjsonschema-2.17.1-py3-none-any.whl (23 kB)\n",
            "Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openvino_telemetry-2024.1.0-py3-none-any.whl (23 kB)\n",
            "Downloading openvino_dev-2024.3.0-16041-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openvino-telemetry, fastjsonschema, numpy, networkx, openvino, onnx, openvino-dev, bitsandbytes, peft\n",
            "  Attempting uninstall: fastjsonschema\n",
            "    Found existing installation: fastjsonschema 2.20.0\n",
            "    Uninstalling fastjsonschema-2.20.0:\n",
            "      Successfully uninstalled fastjsonschema-2.20.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.3\n",
            "    Uninstalling networkx-3.3:\n",
            "      Successfully uninstalled networkx-3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\n",
            "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.43.3 fastjsonschema-2.17.1 networkx-3.1 numpy-1.25.2 onnx-1.15.0 openvino-2024.3.0 openvino-dev-2024.3.0 openvino-telemetry-2024.1.0 peft-0.12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "2360c355c57846c385f877999743290e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install transformers torch onnx openvino-dev[onnx] datasets accelerate openvino peft bitsandbytes trl -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show transformers\n",
        "!pip show accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBCjoB1pC3Ee",
        "outputId": "7d7df07f-b06a-484e-db07-9a05dd35886a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: transformers\n",
            "Version: 4.44.2\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: peft, trl\n",
            "Name: accelerate\n",
            "Version: 0.34.0\n",
            "Summary: Accelerate\n",
            "Home-page: https://github.com/huggingface/accelerate\n",
            "Author: The HuggingFace team\n",
            "Author-email: zach.mueller@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
            "Required-by: peft, trl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers accelerate\n",
        "!pip install transformers[torch] accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PNSkq2pD21N",
        "outputId": "47d36ea9-5ef7-4234-99b0-af71e6f6214e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.44.2\n",
            "Uninstalling transformers-4.44.2:\n",
            "  Successfully uninstalled transformers-4.44.2\n",
            "Found existing installation: accelerate 0.34.0\n",
            "Uninstalling accelerate-0.34.0:\n",
            "  Successfully uninstalled accelerate-0.34.0\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-0.34.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting transformers[torch]\n",
            "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.4.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Using cached accelerate-0.34.0-py3-none-any.whl (324 kB)\n",
            "Using cached transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
            "Installing collected packages: accelerate, transformers\n",
            "Successfully installed accelerate-0.34.0 transformers-4.44.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from random import randrange\n",
        "#from peft import Loraconfig, get_peft_model, AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "ZDZCceeAGcN_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRRamNNSD_Hj",
        "outputId": "b6214a09-3cb3-44b7-909e-101c8980e2b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./my_gpt_distilled_model\")\n",
        "tokenizer.save_pretrained(\"./my_gpt_distilled_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2n4v8ljEDzT",
        "outputId": "17eb282c-4985-4299-df32-dc45623e1b0d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./my_gpt_distilled_model/tokenizer_config.json',\n",
              " './my_gpt_distilled_model/special_tokens_map.json',\n",
              " './my_gpt_distilled_model/vocab.json',\n",
              " './my_gpt_distilled_model/merges.txt',\n",
              " './my_gpt_distilled_model/added_tokens.json',\n",
              " './my_gpt_distilled_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./my_gpt_distilled_model\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./my_gpt_distilled_model\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "class ORTModelForOnnxExport(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)[0]\n",
        "\n",
        "ort_model = ORTModelForOnnxExport(model)\n",
        "\n",
        "dummy_input = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\n",
        "\n",
        "dynamic_axes = {\n",
        "    'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
        "    'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
        "    'output': {0: 'batch_size', 1: 'sequence', 2: 'vocab_size'}\n",
        "}\n",
        "\n",
        "torch.onnx.export(ort_model,\n",
        "                  (dummy_input.input_ids, dummy_input.attention_mask),\n",
        "                  \"gpt_distilled_model.onnx\",\n",
        "                  input_names=['input_ids', 'attention_mask'],\n",
        "                  output_names=['output'],\n",
        "                  dynamic_axes=dynamic_axes,\n",
        "                  opset_version=14,\n",
        "                  do_constant_folding=True)\n",
        "\n",
        "print(\"Model converted to ONNX format successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT0BeIw2EdAV",
        "outputId": "38f46d44-d704-4cb2-acc4-f002edb41d8f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if past_key_values_length > 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model converted to ONNX format successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openvino as ov\n",
        "import os\n",
        "\n",
        "ir_model = ov.convert_model(\"gpt_distilled_model.onnx\",\n",
        "                            input=[ov.PartialShape([1, 4])])\n",
        "\n",
        "ov.save_model(ir_model, \"gpt_distilled_ir_model.xml\")\n",
        "\n",
        "print(\"Model converted to IR format successfully.\")\n",
        "\n",
        "print(\"XML file exists:\", os.path.exists(\"gpt_distilled_ir_model.xml\"))\n",
        "print(\"BIN file exists:\", os.path.exists(\"gpt_distilled_ir_model.bin\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU4XFSAME4HL",
        "outputId": "d2721ef4-87b2-419d-9294-02ab9dbbe470"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model converted to IR format successfully.\n",
            "XML file exists: True\n",
            "BIN file exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('/content/medquad.csv')\n",
        "\n",
        "df = df[['question', 'answer']]"
      ],
      "metadata": {
        "id": "j4hyb0_hFwg6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_df, sample_df = train_test_split(df, test_size=0.1, random_state=42) #using only sample df as the whole dataset is very big"
      ],
      "metadata": {
        "id": "ilRcXfx3G_Nb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df['text'] = 'Question:\\n' + sample_df['question'] + '\\n\\nAnswer:\\n' + sample_df['answer']\n",
        "sample_df.drop(columns=['question','answer'], axis=1,inplace=True)\n",
        "sample_df.to_csv('/content/pre_dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "kSBB8czRHNM5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/pre_dataset.csv')\n",
        "\n",
        "train = Dataset.from_pandas(df)\n",
        "train_dataset_size = len(train)\n",
        "print(train_dataset_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_TXHp60HPny",
        "outputId": "67cffce2-f471-4ed9-852a-4cf83db7772d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfXNFB2eHRiz",
        "outputId": "6c287908-d3c0-4683-8596-268c7bb14ca5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 1642\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, TrainerCallback\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import torch\n",
        "\n",
        "model_name = \"./my_gpt_distilled_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "train_dataset_size = len(train)\n",
        "batch_size = 2\n",
        "steps_per_epoch = train_dataset_size // batch_size\n",
        "\n",
        "target_steps = 300\n",
        "num_epochs = max(3, target_steps // steps_per_epoch)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=num_epochs,\n",
        "    per_device_train_batch_size=2,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,\n",
        ")\n",
        "\n",
        "class PrinterCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if state.is_local_process_zero and logs is not None:\n",
        "            if 'loss' in logs:\n",
        "                print(f\"Step {state.global_step}: Training Loss: {logs['loss']:.5f}\")\n",
        "            else:\n",
        "                print(f\"Step {state.global_step}: Logs: {logs}\")\n",
        "\n",
        "class PositionIdsCapCallback(TrainerCallback):\n",
        "    def on_step_begin(self, args, state, control, **kwargs):\n",
        "        # Get the model directly from the kwargs\n",
        "        model = kwargs.get('model')\n",
        "        if model is not None:\n",
        "            # Get the maximum position id the model can handle\n",
        "            max_position_id = model.config.max_position_embeddings\n",
        "\n",
        "            # Modify the training dataset to cap position ids and token ids\n",
        "            def cap_ids(example):\n",
        "                # Cap input_ids to be within the vocabulary size\n",
        "                example['input_ids'] = [min(id, model.config.vocab_size - 1) for id in example['input_ids']]\n",
        "\n",
        "                # Cap position_ids to be within the maximum position embeddings\n",
        "                max_position_id = model.config.max_position_embeddings - 1\n",
        "                position_ids = list(range(len(example['input_ids'])))\n",
        "                example['position_ids'] = [min(id, max_position_id) for id in position_ids]\n",
        "\n",
        "                return example\n",
        "            if 'trainer' in kwargs and hasattr(kwargs['trainer'], 'train_dataset'):\n",
        "                dataset = kwargs['trainer'].train_dataset\n",
        "                dataset = dataset.map(cap_ids) # Use cap_ids here\n",
        "\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train,\n",
        "    dataset_text_field='text',\n",
        "    max_seq_length=720,\n",
        "    tokenizer=tokenizer,\n",
        "    packing=True,\n",
        "    callbacks=[PrinterCallback(),PositionIdsCapCallback()]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"./fine_tuned_gpt_distilled_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "43cfe6d85fb24707be4bf7e8f62892cf",
            "3932927eff314cba8f8ce51d44462b7a",
            "569e409d5d42456096768733ef2bbdc0",
            "17e17af478c64ab1a35b9edb1af12670",
            "ed689824cfcb4ee5ae7055336416fb7a",
            "4379996f2e5748e093a76304d48baf39",
            "a83c2902ea4247cdb57b6317100f16df",
            "57493f6191554cd28fd076f51b4ccb7c",
            "22f89c9efffd484da82ac73cae6a3985",
            "20734f8822fa4aa7b881873139912e73",
            "5b13cad825994543af322ace3fc408a2"
          ]
        },
        "id": "yntlqM8PHeas",
        "outputId": "5d0689d9-5d9e-4b94-fc7f-8c7046dc506b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43cfe6d85fb24707be4bf7e8f62892cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2858 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1117' max='1116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1116/1116 08:52, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.071800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.880800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.832000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.023000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.045100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.998300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.989500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.872600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.718900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.841200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.889200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>2.571400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>2.651900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>2.774700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.657700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>2.654400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>2.553800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.345100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>2.436700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.297100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>2.547900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>2.371200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>2.420200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>2.495500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.479700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>2.431000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>2.443700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>2.254900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>2.423900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.426600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>2.301700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>2.242200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>2.215700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>2.258900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.218700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>2.190600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>2.269000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>2.130200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>2.027700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.233600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>2.170300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>2.127800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>2.054700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>2.115900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>2.109700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>2.150500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>2.001500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>2.230000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>2.011800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.044800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>2.187800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>2.062400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>2.055600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>2.164900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>2.184800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>2.051000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>2.251800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>1.850100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>2.167000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.054300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>2.075000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>2.278600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>1.947800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>2.127800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.955600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>1.861500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>1.881800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>2.037700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>2.086300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>1.997800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>2.022200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>1.946500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>1.809600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.961700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>1.869900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>1.958400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>1.831100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>1.880600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.766200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>1.971500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>2.028500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>1.983800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>1.917100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>2.089600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>1.958300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>1.963700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>1.876300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>1.892600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.580200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>1.946600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>1.988200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>2.114900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>1.825000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>2.018000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>1.637700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>1.854600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>2.106200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>1.905700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.085900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>1.836600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>1.791800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>2.007600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>1.824600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>1.770600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>1.775200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>1.810500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>1.881400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>1.784400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.796500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>2.003000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10: Training Loss: 3.07180\n",
            "Step 20: Training Loss: 2.88080\n",
            "Step 30: Training Loss: 2.83200\n",
            "Step 40: Training Loss: 3.02300\n",
            "Step 50: Training Loss: 3.04510\n",
            "Step 60: Training Loss: 2.99830\n",
            "Step 70: Training Loss: 2.98950\n",
            "Step 80: Training Loss: 2.87260\n",
            "Step 90: Training Loss: 2.71890\n",
            "Step 100: Training Loss: 2.84120\n",
            "Step 110: Training Loss: 2.88920\n",
            "Step 120: Training Loss: 2.57140\n",
            "Step 130: Training Loss: 2.65190\n",
            "Step 140: Training Loss: 2.77470\n",
            "Step 150: Training Loss: 2.65770\n",
            "Step 160: Training Loss: 2.65440\n",
            "Step 170: Training Loss: 2.55380\n",
            "Step 180: Training Loss: 2.34510\n",
            "Step 190: Training Loss: 2.43670\n",
            "Step 200: Training Loss: 2.29710\n",
            "Step 210: Training Loss: 2.54790\n",
            "Step 220: Training Loss: 2.37120\n",
            "Step 230: Training Loss: 2.42020\n",
            "Step 240: Training Loss: 2.49550\n",
            "Step 250: Training Loss: 2.47970\n",
            "Step 260: Training Loss: 2.43100\n",
            "Step 270: Training Loss: 2.44370\n",
            "Step 280: Training Loss: 2.25490\n",
            "Step 290: Training Loss: 2.42390\n",
            "Step 300: Training Loss: 2.42660\n",
            "Step 310: Training Loss: 2.30170\n",
            "Step 320: Training Loss: 2.24220\n",
            "Step 330: Training Loss: 2.21570\n",
            "Step 340: Training Loss: 2.25890\n",
            "Step 350: Training Loss: 2.21870\n",
            "Step 360: Training Loss: 2.19060\n",
            "Step 370: Training Loss: 2.26900\n",
            "Step 380: Training Loss: 2.13020\n",
            "Step 390: Training Loss: 2.02770\n",
            "Step 400: Training Loss: 2.23360\n",
            "Step 410: Training Loss: 2.17030\n",
            "Step 420: Training Loss: 2.12780\n",
            "Step 430: Training Loss: 2.05470\n",
            "Step 440: Training Loss: 2.11590\n",
            "Step 450: Training Loss: 2.10970\n",
            "Step 460: Training Loss: 2.15050\n",
            "Step 470: Training Loss: 2.00150\n",
            "Step 480: Training Loss: 2.23000\n",
            "Step 490: Training Loss: 2.01180\n",
            "Step 500: Training Loss: 2.04480\n",
            "Step 510: Training Loss: 2.18780\n",
            "Step 520: Training Loss: 2.06240\n",
            "Step 530: Training Loss: 2.05560\n",
            "Step 540: Training Loss: 2.16490\n",
            "Step 550: Training Loss: 2.18480\n",
            "Step 560: Training Loss: 2.05100\n",
            "Step 570: Training Loss: 2.25180\n",
            "Step 580: Training Loss: 1.85010\n",
            "Step 590: Training Loss: 2.16700\n",
            "Step 600: Training Loss: 2.05430\n",
            "Step 610: Training Loss: 2.07500\n",
            "Step 620: Training Loss: 2.27860\n",
            "Step 630: Training Loss: 1.94780\n",
            "Step 640: Training Loss: 2.12780\n",
            "Step 650: Training Loss: 1.95560\n",
            "Step 660: Training Loss: 1.86150\n",
            "Step 670: Training Loss: 1.88180\n",
            "Step 680: Training Loss: 2.03770\n",
            "Step 690: Training Loss: 2.08630\n",
            "Step 700: Training Loss: 2.02900\n",
            "Step 710: Training Loss: 1.99780\n",
            "Step 720: Training Loss: 2.02220\n",
            "Step 730: Training Loss: 1.94650\n",
            "Step 740: Training Loss: 1.80960\n",
            "Step 750: Training Loss: 1.96170\n",
            "Step 760: Training Loss: 1.86990\n",
            "Step 770: Training Loss: 1.95840\n",
            "Step 780: Training Loss: 1.83110\n",
            "Step 790: Training Loss: 1.88060\n",
            "Step 800: Training Loss: 1.76620\n",
            "Step 810: Training Loss: 1.97150\n",
            "Step 820: Training Loss: 2.02850\n",
            "Step 830: Training Loss: 1.98380\n",
            "Step 840: Training Loss: 1.91710\n",
            "Step 850: Training Loss: 2.08960\n",
            "Step 860: Training Loss: 1.95830\n",
            "Step 870: Training Loss: 1.96370\n",
            "Step 880: Training Loss: 1.87630\n",
            "Step 890: Training Loss: 1.89260\n",
            "Step 900: Training Loss: 1.58020\n",
            "Step 910: Training Loss: 1.94660\n",
            "Step 920: Training Loss: 1.98820\n",
            "Step 930: Training Loss: 2.11490\n",
            "Step 940: Training Loss: 1.82500\n",
            "Step 950: Training Loss: 2.01800\n",
            "Step 960: Training Loss: 1.63770\n",
            "Step 970: Training Loss: 1.85460\n",
            "Step 980: Training Loss: 2.10620\n",
            "Step 990: Training Loss: 1.90570\n",
            "Step 1000: Training Loss: 2.08590\n",
            "Step 1010: Training Loss: 1.83660\n",
            "Step 1020: Training Loss: 1.79180\n",
            "Step 1030: Training Loss: 2.00760\n",
            "Step 1040: Training Loss: 1.82460\n",
            "Step 1050: Training Loss: 1.77060\n",
            "Step 1060: Training Loss: 1.77520\n",
            "Step 1070: Training Loss: 1.81050\n",
            "Step 1080: Training Loss: 1.88140\n",
            "Step 1090: Training Loss: 1.78440\n",
            "Step 1100: Training Loss: 1.79650\n",
            "Step 1110: Training Loss: 2.00300\n",
            "Step 1116: Logs: {'train_runtime': 542.9668, 'train_samples_per_second': 4.105, 'train_steps_per_second': 2.055, 'total_flos': 409521415127040.0, 'train_loss': 2.1757565296678987, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1116' max='1116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1116/1116 09:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.071800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.880800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.832000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.023000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.045100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.998300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.989500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.872600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.718900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.841200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.889200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>2.571400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>2.651900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>2.774700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.657700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>2.654400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>2.553800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.345100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>2.436700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.297100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>2.547900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>2.371200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>2.420200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>2.495500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.479700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>2.431000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>2.443700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>2.254900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>2.423900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.426600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>2.301700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>2.242200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>2.215700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>2.258900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.218700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>2.190600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>2.269000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>2.130200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>2.027700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.233600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>2.170300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>2.127800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>2.054700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>2.115900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>2.109700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>2.150500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>2.001500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>2.230000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>2.011800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.044800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>2.187800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>2.062400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>2.055600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>2.164900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>2.184800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>2.051000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>2.251800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>1.850100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>2.167000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.054300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>2.075000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>2.278600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>1.947800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>2.127800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.955600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>1.861500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>1.881800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>2.037700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>2.086300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>1.997800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>2.022200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>1.946500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>1.809600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.961700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>1.869900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>1.958400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>1.831100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>1.880600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.766200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>1.971500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>2.028500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>1.983800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>1.917100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>2.089600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>1.958300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>1.963700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>1.876300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>1.892600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.580200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>1.946600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>1.988200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>2.114900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>1.825000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>2.018000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>1.637700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>1.854600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>2.106200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>1.905700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.085900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>1.836600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>1.791800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>2.007600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>1.824600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>1.770600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>1.775200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>1.810500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>1.881400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>1.784400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.796500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>2.003000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_gpt_distilled_model\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_gpt_distilled_model\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "class ORTModelForOnnxExport(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)[0]\n",
        "\n",
        "ort_model = ORTModelForOnnxExport(model)\n",
        "\n",
        "dummy_input = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\n",
        "\n",
        "dynamic_axes = {\n",
        "    'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
        "    'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
        "    'output': {0: 'batch_size', 1: 'sequence', 2: 'vocab_size'}\n",
        "}\n",
        "\n",
        "torch.onnx.export(ort_model,\n",
        "                  (dummy_input.input_ids, dummy_input.attention_mask),\n",
        "                  \"fine_tuned_gpt_distilled_model.onnx\",\n",
        "                  input_names=['input_ids', 'attention_mask'],\n",
        "                  output_names=['output'],\n",
        "                  dynamic_axes=dynamic_axes,\n",
        "                  opset_version=14,\n",
        "                  do_constant_folding=True)\n",
        "\n",
        "print(\"Fine-tuned model converted to ONNX format successfully.\")"
      ],
      "metadata": {
        "id": "RJlZEaSOSxNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "567dab10-8c5a-436d-fe0a-d96fdbff802e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned model converted to ONNX format successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openvino.tools import mo\n",
        "ov_model = mo.convert_model(\"fine_tuned_gpt_distilled_model.onnx\", input=[('input_ids', [1, -1]), ('attention_mask', [1, -1])], output=['output'])\n",
        "from openvino.runtime import serialize\n",
        "serialize(ov_model, \"fine_tuned_gpt_distilled_ir_model.xml\")\n",
        "print(\"Fine-tuned model converted to IR format successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXVDaTa_s6SR",
        "outputId": "45940321-190c-491b-d364-80a27911948a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
            "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
            "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
            "Fine-tuned model converted to IR format successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openvino.runtime as ov\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_gpt_distilled_model\")\n",
        "\n",
        "core = ov.Core()\n",
        "model = core.read_model(\"fine_tuned_gpt_distilled_ir_model.xml\")\n",
        "compiled_model = core.compile_model(model, \"CPU\")\n",
        "\n",
        "infer_request = compiled_model.create_infer_request()\n",
        "\n",
        "def generate_text(prompt, max_length=200): # The maximum length is 200; the answer may be short.\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.numpy()\n",
        "    attention_mask = tokenizer(prompt, return_tensors=\"pt\").attention_mask.numpy()\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
        "\n",
        "        results = infer_request.infer(inputs)\n",
        "        logits = results[next(iter(results))]\n",
        "\n",
        "        next_token_id = np.argmax(logits[0, -1, :])\n",
        "\n",
        "        input_ids = np.concatenate([input_ids, np.array([[next_token_id]])], axis=1)\n",
        "        attention_mask = np.concatenate([attention_mask, np.array([[1]])], axis=1)\n",
        "\n",
        "        if next_token_id == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "prompt = \"What is (are) Schwannomatosis ?\"\n",
        "generated_text = generate_text(prompt)\n",
        "print(\"Generated text:\", generated_text)\n",
        "\n",
        "#True answer: Schwannomatosis is a rare form of neurofibromatosis that is primarily characterized by multiple schwannomas (benign tumors of the nervous system) in the absence of bilateral (affecting both sides) vestibular schwannomas. Signs and symptoms of the condition vary based on the size, location and number of schwannomas but may include pain; numbness; tingling; and/or weakness in the fingers and toes. Inherited forms of the disorder account for only 15 percent of all cases. In some of these families, schwannomatosis is caused by changes (mutations) in the SMARCB1 or LZTR1 genes; in other cases, the exact underlying cause is unknown. When inherited, the condition is passed down in an autosomal dominant manner with highly variable expressivity and reduced penetrance. Treatment is based on the signs and symptoms present in each person but may include medications and/or surgery."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4mUfcsZtBPq",
        "outputId": "ee2256d8-1d53-4b13-b878-53e83eb2a521"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: What is (are) Schwannomatosis?\n",
            "\n",
            "Answer:\n",
            "Schwannomatosis is a condition that affects the body's ability to make enough fat to keep the body's energy level at a normal level. The body's ability to make enough fat to keep the body's energy level at a normal level is called the \"hypotonia\" or \"hypotonia.\" The body's ability to make enough fat to keep the body's energy level at a normal level is called the \"hypotonia\" or \"hypotonia.\" The body's ability to make enough fat to keep the body's energy level at a normal level is called the \"hypotonia\" or \"hypotonia.\" The body's ability to make enough fat to keep the body's energy level at a normal level is called the \"hypotonia\" or \"hypotonia.\" The body's ability to make enough fat to keep the body's energy level at a normal level is called the \"hypotonia\" or \"hypotonia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "fine_tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_gpt_distilled_model\")\n",
        "fine_model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_gpt_distilled_model\")"
      ],
      "metadata": {
        "id": "14GlcKrava-N"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_model.cuda() # Using GPU for generating text\n",
        "\n",
        "prompt = \"What are the symptoms of Crome syndrome ?\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "inputs = fine_tokenizer(prompt, return_tensors='pt', truncation=True, padding=True)\n",
        "input_ids = inputs.input_ids.to(device)\n",
        "attention_mask = inputs.attention_mask.to(device)\n",
        "\n",
        "outputs = fine_model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.6,\n",
        "    pad_token_id=fine_tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pz5GbY5gcra",
        "outputId": "654558bb-3fce-4195-8667-53cad98ea475"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = fine_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVfIFM39gmYr",
        "outputId": "67ac152e-89a1-4a71-8ca8-c74d5a028bd2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are the symptoms of Crome syndrome?\n",
            "\n",
            "Answer:\n",
            "Crome syndrome is a rare disorder that affects the brain and spinal cord. It is caused by mutations in the SLC1 gene. The SLC1 gene provides instructions for making a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called a protein called\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.metrics import accuracy_score\n",
        "import psutil\n",
        "import torch\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# 1. Inference Time (Latency)\n",
        "start_time = time.time()\n",
        "generated_text = generated_text = fine_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "end_time = time.time()\n",
        "inference_time = end_time - start_time\n",
        "print(f\"Inference Time: {inference_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ujQf83ngomn",
        "outputId": "9fe31373-a7ef-44f2-81f5-4c7d56f9249e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time: 0.0008 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Throughput\n",
        "num_samples = 10\n",
        "start_time = time.time()\n",
        "for _ in range(num_samples):\n",
        "    generated_text = generated_text = fine_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "end_time = time.time()\n",
        "throughput = num_samples / (end_time - start_time)\n",
        "print(f\"Throughput: {throughput:.4f} samples/second\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfN07QrWgtMi",
        "outputId": "330d1f1a-3dcd-41c0-8913-c53e1384bffc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Throughput: 1628.7294 samples/second\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Model Load Time\n",
        "start_time = time.time()\n",
        "core = ov.Core()\n",
        "model = core.read_model(\"fine_tuned_gpt_distilled_ir_model.xml\")\n",
        "compiled_model = core.compile_model(model, \"CPU\")\n",
        "end_time = time.time()\n",
        "model_load_time = end_time - start_time\n",
        "print(f\"Model Load Time: {model_load_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnpiQxhxgv7Y",
        "outputId": "ddbcae23-d1d7-4372-a866-869cb5669540"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Load Time: 2.2316 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. CPU Utilization\n",
        "cpu_percent = psutil.cpu_percent(interval=1)  # Check CPU utilization over 1 second\n",
        "print(f\"CPU Utilization: {cpu_percent}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-Q1_ArbgyYO",
        "outputId": "b6656dbb-8535-40f8-d0bf-e49dad16789b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Utilization: 54.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. BLEU Score\n",
        "question = \"What are the symptoms of Crome syndrome ?\"\n",
        "answer = \"\"\"What are the signs and symptoms of Crome syndrome? The Human Phenotype Ontology provides the following list of signs and symptoms for Crome syndrome.\n",
        "If the information is available, the table below includes how often the symptom is seen in people with this condition. You can use the MedlinePlus Medical Dictionary to look up the definitions for these medical terms. Signs and Symptoms Approximate number of patients (when available) Abnormality of the renal tubule 90% Aplasia/Hypoplasia of the cerebellum 90% Cataract 90% Cognitive impairment 90% Encephalitis 90% Seizures 90% Nystagmus 50% Acute tubular necrosis -\n",
        "Autosomal recessive inheritance - Cerebellar dysplasia - Congenital cataract - Encephalopathy - Intellectual disability - Short stature - The Human Phenotype Ontology (HPO) has collected information on how often a sign or symptom occurs in a condition. Much of this information comes from Orphanet, a European rare disease database.\n",
        " The frequency of a sign or symptom is usually listed as a rough estimate of the percentage of patients who have that feature. The frequency may also be listed as a fraction.\n",
        " The first number of the fraction is how many people had the symptom, and the second number is the total number of people who were examined in one study. For example, a frequency of 25/25 means that in a study of 25 people all patients were found to have that symptom.\n",
        " Because these frequencies are based on a specific study, the fractions may be different if another group of patients are examined. Sometimes, no information on frequency is available. In these cases, the sign or symptom may be rare or common.\"\"\"\n",
        "\n",
        "def generate_text_openvino(prompt, max_length=200):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.numpy()\n",
        "    attention_mask = tokenizer(prompt, return_tensors=\"pt\").attention_mask.numpy()\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
        "        results = infer_request.infer(inputs)\n",
        "        logits = results[next(iter(results))]\n",
        "        next_token_id = np.argmax(logits[0, -1, :])\n",
        "\n",
        "        input_ids = np.concatenate([input_ids, np.array([[next_token_id]])], axis=1)\n",
        "        attention_mask = np.concatenate([attention_mask, np.array([[1]])], axis=1)\n",
        "\n",
        "        if next_token_id == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "generated_text = generate_text_openvino(question)\n",
        "reference = [answer.split()]\n",
        "candidate = generated_text.split()\n",
        "bleu_score = sentence_bleu(reference, candidate)\n",
        "print(f\"BLEU Score: {bleu_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9BS0m9Yg5Wj",
        "outputId": "fff8665d-f241-4895-a228-2755277fed83"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.016202384413105144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Fine-tuning Time\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "end_time = time.time()\n",
        "fine_tuning_time = end_time - start_time\n",
        "print(f\"Fine-tuning Time: {fine_tuning_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Kisk2K_Lg74j",
        "outputId": "c0081abc-0aac-4bf2-c213-8222ecc0e780"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1116' max='1116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1116/1116 10:54, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.012000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.921600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.740800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.736300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.987500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.923700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.131200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.083100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.702200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.899600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.947200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.672300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.886700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.712600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.921300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.870800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.837100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.600100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.854600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.626800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.892500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.674900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.848000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.936400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.916800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.883400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.943400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.643800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.973700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.980200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.795400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.784700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.732800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.819200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.766700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.728400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.874200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.777700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>1.672700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.887800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>1.838800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>1.773300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>1.765400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.804400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.769500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.769300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>1.683300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>1.942300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>1.691000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.700600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>1.906700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>1.807800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>1.748700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>1.892500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.892400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>1.797300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>2.006100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>1.584300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>1.916700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.796300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>1.801400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>2.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>1.690700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>1.870200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.701800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>1.616700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>1.624400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>1.783000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>1.846400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.832300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>1.783900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>1.780600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>1.746800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>1.594300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.733500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>1.631200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>1.699800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>1.611400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>1.637600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.545500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>1.760800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>1.812100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>1.754000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>1.650300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.854500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>1.752800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>1.752300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>1.675200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>1.679700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.377900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>1.739100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>1.775300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>1.909900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>1.642300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>1.828500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>1.411600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>1.622600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>1.930400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>1.687300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.893900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>1.658800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>1.580500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>1.770800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>1.608300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>1.586600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>1.575100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>1.629100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>1.719100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>1.572900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.608900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>1.804000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10: Training Loss: 2.01200\n",
            "Step 20: Training Loss: 1.92160\n",
            "Step 30: Training Loss: 1.74080\n",
            "Step 40: Training Loss: 1.73630\n",
            "Step 50: Training Loss: 1.98750\n",
            "Step 60: Training Loss: 1.92370\n",
            "Step 70: Training Loss: 2.13120\n",
            "Step 80: Training Loss: 2.08310\n",
            "Step 90: Training Loss: 1.70220\n",
            "Step 100: Training Loss: 1.89960\n",
            "Step 110: Training Loss: 1.94720\n",
            "Step 120: Training Loss: 1.67230\n",
            "Step 130: Training Loss: 1.88670\n",
            "Step 140: Training Loss: 1.71260\n",
            "Step 150: Training Loss: 1.92130\n",
            "Step 160: Training Loss: 1.87080\n",
            "Step 170: Training Loss: 1.83710\n",
            "Step 180: Training Loss: 1.60010\n",
            "Step 190: Training Loss: 1.85460\n",
            "Step 200: Training Loss: 1.62680\n",
            "Step 210: Training Loss: 1.89250\n",
            "Step 220: Training Loss: 1.67490\n",
            "Step 230: Training Loss: 1.84800\n",
            "Step 240: Training Loss: 1.93640\n",
            "Step 250: Training Loss: 1.91680\n",
            "Step 260: Training Loss: 1.88340\n",
            "Step 270: Training Loss: 1.94340\n",
            "Step 280: Training Loss: 1.64380\n",
            "Step 290: Training Loss: 1.97370\n",
            "Step 300: Training Loss: 1.98020\n",
            "Step 310: Training Loss: 1.79540\n",
            "Step 320: Training Loss: 1.78470\n",
            "Step 330: Training Loss: 1.73280\n",
            "Step 340: Training Loss: 1.81920\n",
            "Step 350: Training Loss: 1.76670\n",
            "Step 360: Training Loss: 1.72840\n",
            "Step 370: Training Loss: 1.87420\n",
            "Step 380: Training Loss: 1.77770\n",
            "Step 390: Training Loss: 1.67270\n",
            "Step 400: Training Loss: 1.88780\n",
            "Step 410: Training Loss: 1.83880\n",
            "Step 420: Training Loss: 1.77330\n",
            "Step 430: Training Loss: 1.76540\n",
            "Step 440: Training Loss: 1.80440\n",
            "Step 450: Training Loss: 1.76950\n",
            "Step 460: Training Loss: 1.76930\n",
            "Step 470: Training Loss: 1.68330\n",
            "Step 480: Training Loss: 1.94230\n",
            "Step 490: Training Loss: 1.69100\n",
            "Step 500: Training Loss: 1.70060\n",
            "Step 510: Training Loss: 1.90670\n",
            "Step 520: Training Loss: 1.80780\n",
            "Step 530: Training Loss: 1.74870\n",
            "Step 540: Training Loss: 1.89250\n",
            "Step 550: Training Loss: 1.89240\n",
            "Step 560: Training Loss: 1.79730\n",
            "Step 570: Training Loss: 2.00610\n",
            "Step 580: Training Loss: 1.58430\n",
            "Step 590: Training Loss: 1.91670\n",
            "Step 600: Training Loss: 1.79630\n",
            "Step 610: Training Loss: 1.80140\n",
            "Step 620: Training Loss: 2.01640\n",
            "Step 630: Training Loss: 1.69070\n",
            "Step 640: Training Loss: 1.87020\n",
            "Step 650: Training Loss: 1.70180\n",
            "Step 660: Training Loss: 1.61670\n",
            "Step 670: Training Loss: 1.62440\n",
            "Step 680: Training Loss: 1.78300\n",
            "Step 690: Training Loss: 1.84640\n",
            "Step 700: Training Loss: 1.83230\n",
            "Step 710: Training Loss: 1.78390\n",
            "Step 720: Training Loss: 1.78060\n",
            "Step 730: Training Loss: 1.74680\n",
            "Step 740: Training Loss: 1.59430\n",
            "Step 750: Training Loss: 1.73350\n",
            "Step 760: Training Loss: 1.63120\n",
            "Step 770: Training Loss: 1.69980\n",
            "Step 780: Training Loss: 1.61140\n",
            "Step 790: Training Loss: 1.63760\n",
            "Step 800: Training Loss: 1.54550\n",
            "Step 810: Training Loss: 1.76080\n",
            "Step 820: Training Loss: 1.81210\n",
            "Step 830: Training Loss: 1.75400\n",
            "Step 840: Training Loss: 1.65030\n",
            "Step 850: Training Loss: 1.85450\n",
            "Step 860: Training Loss: 1.75280\n",
            "Step 870: Training Loss: 1.75230\n",
            "Step 880: Training Loss: 1.67520\n",
            "Step 890: Training Loss: 1.67970\n",
            "Step 900: Training Loss: 1.37790\n",
            "Step 910: Training Loss: 1.73910\n",
            "Step 920: Training Loss: 1.77530\n",
            "Step 930: Training Loss: 1.90990\n",
            "Step 940: Training Loss: 1.64230\n",
            "Step 950: Training Loss: 1.82850\n",
            "Step 960: Training Loss: 1.41160\n",
            "Step 970: Training Loss: 1.62260\n",
            "Step 980: Training Loss: 1.93040\n",
            "Step 990: Training Loss: 1.68730\n",
            "Step 1000: Training Loss: 1.89390\n",
            "Step 1010: Training Loss: 1.65880\n",
            "Step 1020: Training Loss: 1.58050\n",
            "Step 1030: Training Loss: 1.77080\n",
            "Step 1040: Training Loss: 1.60830\n",
            "Step 1050: Training Loss: 1.58660\n",
            "Step 1060: Training Loss: 1.57510\n",
            "Step 1070: Training Loss: 1.62910\n",
            "Step 1080: Training Loss: 1.71910\n",
            "Step 1090: Training Loss: 1.57290\n",
            "Step 1100: Training Loss: 1.60890\n",
            "Step 1110: Training Loss: 1.80400\n",
            "Step 1116: Logs: {'train_runtime': 655.2631, 'train_samples_per_second': 3.402, 'train_steps_per_second': 1.703, 'total_flos': 409521415127040.0, 'train_loss': 1.7710496093209926, 'epoch': 3.0}\n",
            "Fine-tuning Time: 656.1447 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = {\n",
        "    \"Inference Time (Latency)\": inference_time,\n",
        "    \"Throughput\": throughput,\n",
        "    \"Model Load Time\": model_load_time,\n",
        "    \"CPU Utilization\": cpu_percent,\n",
        "    \"BLEU Score\": bleu_score,\n",
        "    \"Fine-tuning Time\": fine_tuning_time\n",
        "}\n",
        "\n",
        "with open(\"GPT Distilled metrics.txt\", \"w\") as f:\n",
        "    for key, value in metrics.items():\n",
        "        f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "print(\"Metrics saved to metrics.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cygQrtJ1hLrG",
        "outputId": "d687cb72-2e1a-4600-9aa0-03165093a5e7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics saved to metrics.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tC9SC_pLjxwJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}